{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inclusive Speech Technology Lab - Week 2\n",
    "\n",
    "## Acoustic Features for Inclusive Speech Processing\n",
    "\n",
    "**The goal of this lab is to reflect on what kind of information is included in speech and what kind of acoustic features capture task-specific information. In other words, which acoustic feature(s) is best for what speech processing task and link this to speech as a communication signal.** In this lab you will learn about various acoustic features. First, you will extract different acoustic features from speech files. Throughout this lab, you will be using two databases, the German emotions dataset [Emo-DB](https://github.com/audeering/emodb), and the [Delft Database of EEG Recordings of Dutch Articulated and Imagined Speech (DAIS)](https://pure.tudelft.nl/ws/portalfiles/portal/157666992/DAIS_The_Delft_Database_of_EEG_Recordings_of_Dutch_Articulated_and_Imagined_Speech.pdf) dataset. You will extract features from each dataset, and cluster them using k-means clustering, with the goal of finding clusters that can predict emotion (for the Emo-DB dataset) and vowels (for the DAIS dataset). You will analyze how effective different acoustic features are at achieving this goal. The lab consist of the following parts:\n",
    "\n",
    "1. Loading the Datasets\n",
    "2. Acoustic Feature Extraction\n",
    "3. K-Means Clustering\n",
    " \n",
    "Throughout this lab you will be asked to write code in this notebook, and you will also be asked to **reflect on your results which are to be written down in a group report.** The report should only focus on this lab. **For more information on the report, refer to the Inclusive Speech Technology Brightspace page, in the `Lab` section.**\n",
    "\n",
    "**We provide reflection questions in this notebook.** Sections where we ask you to implement a coding exercise will be marked with a <i style='color: #468fea; font-size: 15px;' class=\"fa fa-code\" aria-hidden=\"true\"></i> symbol. Sections where we ask reflection questions will be marked with a <i style='color: #468fea; font-size: 15px;' class=\"fa fa-file-text\" aria-hidden=\"true\"></i> symbol. Please answer all questions. However, keep in mind that **just** answering the questions in this notebook will not result in a good quality report.\n",
    "\n",
    "**There is no \"correct\" way of completing this lab!** It is, however, important that you can justify why you make certain choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Get Started\n",
    "\n",
    "Before getting started with the lab, you should import all of the necessary libraries that you will be using. **Run the code block below to install the necessary libraries for this lab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the libraries using pip\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install seaborn\n",
    "%pip install librosa\n",
    "%pip install opensmile\n",
    "%pip install audb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have installed the libraries, **run the code block below to import them into the notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions\n",
    "\n",
    "# Imports for plotting and data manipulatin\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Imports for audio processing\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import audb\n",
    "import audiofile\n",
    "import opensmile\n",
    "import os\n",
    "\n",
    "# Imports for clustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Datasets\n",
    "\n",
    "In this lab you will be working with two databases: the German emotions dataset [Emo-DB](https://github.com/audeering/emodb), which contains speech spoken with different emotions, and the [Delft Database of EEG Recordings of Dutch Articulated and Imagined Speech (DAIS)](https://pure.tudelft.nl/ws/portalfiles/portal/157666992/DAIS_The_Delft_Database_of_EEG_Recordings_of_Dutch_Articulated_and_Imagined_Speech.pdf) dataset, which contains speech of vowels. This section will explain how to load each dataset.\n",
    "\n",
    "Beginning with the German emotions dataset, this can be download and loaded using one command: `emodb = audb.load('emodb')`. This command will download the dataset for you. For more documentation on using `audb.load(...)`, refer to the [documentation](https://audeering.github.io/audb/load.html). To access one specific file, you can use the following command:\n",
    "```\n",
    "db = audb.load(\n",
    "    'emodb',\n",
    "    version='1.4.1',\n",
    "    format='wav',\n",
    "    mixdown=True,\n",
    "    sampling_rate=16000,\n",
    "    media='wav/14.*A.*\\.wav',  # here you can choose which subset of files to use (for interpretation of the filenames see emodb documentation)\n",
    "    full_path=False,\n",
    "    verbose=False,\n",
    ")\n",
    "```\n",
    "\n",
    "The DAIS dataset can be downloaded from Brightspace. The file is called `dais.zip` and can be found under `Content` > `Labs` > `Week 2`. Download this file and unzip it. Once you have downloaded the files to your local machine, you can load the files as you would normally with Python. For example, you can get file paths using `os.walk(...)`, and you can load the speech file using `signal, sampling_rate = audiofile.read(file_path)`. **Once you are done with this lab, make sure to delete the DAIS dataset from your local machine.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Acoustic Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the next part of the lab, you will practice extracting acoustic features using Emo-DB.** You will be using [openSMILE](https://audeering.github.io/opensmile-python/usage.html) to do so. The goal of this section is to become comfortable with extracting acoustic features and understanding them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Extract & Explore Features of a Single Speech File\n",
    "\n",
    "**First you will take a look at a single speech file and analyse it using openSMILE.** Below, you will load the Emo-DB dataset that you will be using. Loading the dataset will allow you to work with the speech files in this notebook. We provide you the code to do so in the code block below. We also give you code to display a specific speech file from the dataset, which you can listen to to get an understanding of the speech that the dataset contains. **Run the code block below to load your dataset and display a speech file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load emodb database\n",
    "db = audb.load(\n",
    "    'emodb',\n",
    "    version='1.4.1',\n",
    "    format='wav',\n",
    "    mixdown=True,\n",
    "    sampling_rate=16000,\n",
    "    media=r'wav/14.*A.*\\.wav',\n",
    "    full_path=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Read speech file\n",
    "file = os.path.join(db.root, db.files[1])\n",
    "signal, sampling_rate = audiofile.read(\n",
    "    file,\n",
    "    duration=10,\n",
    "    always_2d=True,\n",
    ")\n",
    "\n",
    "# Play audio\n",
    "Audio(data=signal, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you will extract different acoustic features from the speech file you just loaded.** By extracting acoustic features from speech signals, we quantify and measure certain aspects of the speech signal that help us analyze the signal. To extract acoustic features, you will use `opensmile.Smile`. This object allows you to extract acoustic features according to a `feature_set` and `feature_level`. For this lab, we will be using the acoustic feature set `opensmile.FeatureSet.eGeMAPSv02`, which corresponds to The Extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS). eGeMAPS is further explained in [this paper](https://sail.usc.edu/publications/files/eyben-preprinttaffc-2015.pdf).\n",
    "\n",
    "To store the acoustic features, we will use Pandas, specifically the [`pd.DataFrame()` object](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). Each frame (row) corresponds to a segment of speech. Columns represent different acoustic features, or other important information such as start and end times of the frame (row).\n",
    "\n",
    "First, we will focus on getting all the eGeMAPS features. We provide the code to extract and display the eGeMAPS features, which you can run below. For guidance on how to work with acoustic feature extraction in `OpenSMILE`, you can refer to [this](https://audeering.github.io/opensmile-python/usage.html) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the OpenSmile feature extractor and process signal\n",
    "smile = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "    feature_level=opensmile.FeatureLevel.LowLevelDescriptors,\n",
    ")\n",
    "\n",
    "features = smile.process_signal(\n",
    "    signal,\n",
    "    sampling_rate\n",
    ")\n",
    "\n",
    "# Display all features\n",
    "pd.set_option('display.max_columns', None)\n",
    "features_df = features.copy()\n",
    "features_df.columns = [''.join(col).strip() for col in features_df.columns.values]\n",
    "features_df = features_df.reset_index()\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that eGeMAPS computes a lot of acoustic features. **Throughout this lab you will be focusing on four of these acoustic features: pitch, jitter, loudness, and MFCCs.** We expect you to understand what acoustic information each of these acoustic features capture. Please refer to the [eGeMAPS documentation](https://sail.usc.edu/publications/files/eyben-preprinttaffc-2015.pdf) in case of questions about these acoustic features.\n",
    "\n",
    "<i style='color: #468fea; font-size: 20px;' class=\"fa fa-code\" aria-hidden=\"true\"></i> For this next code block, you **should filter your data such that only the four acoustic features you want to work with are present.** Note that MFCCs have several orders, and eGeMAPS provides MFCC orders 1-4. You should choose which MFCC order or combination of orders you want to use for your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Extract & Explore Features of the Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a better understand of the underlying data, we will take a look at the full dataset. Load the complete Emo-DB dataset. Then, **complete the feature extraction on the complete Emo-DB dataset.** You should do this using the same method as you used before: define the openSMILE feature extractor and process the speech signals. \n",
    "\n",
    "In the previous section, you had multiple frames (rows) for a single speech file. Now, you will also have multiple files to work with, each of which will be split into multiple frames. Thus, **consider carefully how you will organize your data.** Tips: you will likely want to add a new column which keeps track of which file a speech signal corresponds to. You may also want to combine frames from the same file, such that each file has only one frame.\n",
    "\n",
    "Make sure you also filter the acoustic features such that only the necessary acoustic features are present!\n",
    "\n",
    "<i style='color: #468fea; font-size: 20px;' class=\"fa fa-code\" aria-hidden=\"true\"></i> Load the audio from the Emo-DB dataset, extract the four acoustic features for each file in `emodb.files`, and store the features in a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the complete Emo-DB dataset\n",
    "emodb = audb.load('emodb')\n",
    "\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have a dataframe that contains all of the extracted acoustic features for each speech file. Next, you will merge your acoustic features with the emotion labels for the speech files. This will allow you to have all acoustic features and emotion information for a speech file in one place (or put differently: each acoustic feature now has an emotion label), which will make processing the data easier. First, you need to get the emotion labels corresponding to each speech file. To do so, we provide you code that loads the emotion labels into a dataframe object, stored in the `merged_df` variable.\n",
    "\n",
    "**Run the code block below to get the emotion labels for the speech files.** Read the table that is output, so you understand the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for the ground truth data / original CSV files\n",
    "path = emodb.root  # Just use it directly\n",
    "emotion_file = os.path.join(path, 'db.emotion.csv')\n",
    "files_file = os.path.join(path, 'db.files.csv')\n",
    "speaker_file = os.path.join(path, 'db.speaker.csv')\n",
    "\n",
    "# Load the CSV files\n",
    "emotion_df = pd.read_csv(emotion_file)\n",
    "files_df = pd.read_csv(files_file)\n",
    "speaker_df = pd.read_csv(speaker_file)\n",
    "\n",
    "# Merge the dataframes\n",
    "merged_df = pd.merge(emotion_df, files_df, on='file')\n",
    "merged_df = pd.merge(merged_df, speaker_df, on='speaker')\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have two dataframes, one that contains the features per each file, and one that contains the emotion labels per each file. **Combine these two dataframes into one dataframe that contains both the acoustic features and the emotion label for each file.**\n",
    "\n",
    "<i style='color: #468fea; font-size: 20px;' class=\"fa fa-code\" aria-hidden=\"true\"></i> Combine the emotion label and acoustic feature frames for each speech file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a second to reflect on the feature data you have produced in the table above.**\n",
    "\n",
    "<i style='color: #468fea; font-size: 20px;' class=\"fa fa-file-text\" aria-hidden=\"true\"></i> Why are there multiple rows in the table? Why do the values of an acoustic feature (column) fluctuate over the different rows? Why do the different columns have different values? Why do you think your features might be useful for analyzing speech? Which MFCC order(s) did you choose to work with, and why?\n",
    "\n",
    "If you do not know the answers to these questions, please revise the lecture content on acoustic features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Means Clustering\n",
    "\n",
    "### 2.1 Clustering Emotions\n",
    "Now that you have extracted the acoustic features from the emotions database, you will apply k-means clustering to the acoustic features dataframe that you computed in the previous section to investigate how well the acoustic features are able to cluster together speech files that have the same emotion label. First, you should **compute the clusters for each of the four acoustic features, and you should visualize these clusters in a plot for each acoustic feature separately**. Visualizing the plots will help you confirm that your implementation of k-means clustering is correct. We have imported the `KMeans` from [scikit-learn](https://scikit-learn.org/1.5/index.html), which will perform the clustering for you. **Note that you should use seven clusters, since there are seven emotions in the dataset(!)**\n",
    "\n",
    "There are several things you should consider while clustering. First: since the acoustic features are extracted per frame, you should calculate statistical summaries (e.g., mean, standard deviation) for each acoustic feature across the entire speech file to reduce dimensionality. Second, you should standardise the acoustic features to have zero mean and unit variance, since K-Means is sensitive to the scale of the data points.\n",
    "\n",
    "**You should produce four one-dimensional cluster plots, one for each acoustic feature.** Since your clusters are one-dimensional, your plots should show a single line with several colored dots. Each color represents a cluster of acoustic features that your k-means algorithm found.\n",
    "\n",
    "<i style='color: #468fea; font-size: 20px;' class=\"fa fa-code\" aria-hidden=\"true\"></i> Cluster the acoustic features you extracted above, and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your k-means clustering was correct, your plots should contain seven clusters. You may notice that your plots are hard to interpret as the clusters do not have any information on the emotion labels.\n",
    "\n",
    "To understand how well each acoustic feature is in capturing a specific emotion, you can create a heatmap. A heatmap shows how many times each emotion appears in each cluster. The more often an emotion appears in a cluster, the better the acoustic feature is in capturing important information about that emotion. Your heat map should plot emotions on the x-axis, and cluster indices on the y-axis. To create this heat map, you can combine your cluster data with the `merged_df` dataframe that we provide you in part 1.2 of this lab. To plot the heat map, you can use `sns.heatmap`. \n",
    "\n",
    "<i style='color: #468fea; font-size: 20px;' class=\"fa fa-code\" aria-hidden=\"true\"></i> Create and plot a heat map that represents the distribution of emotions per cluster. You should use the clusters you calculated in the previous code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of analyzing how emotions relate to your clusters would be to calculate the cluster purity score. The cluster purity tells you the degree to which the clusters contain a single class (emotion), and is given by the following formula: \n",
    "$$\n",
    "\\text{purity} = \\frac{1}{N}\\sum_{m\\in M}\\max_{d\\in D}|m \\cap d|\n",
    "$$\n",
    "where $M$ is the set of clusters, $D$ is the set of classes (emotions), and $N$ is the number of data points. **Use this formula to calculate the cluster purity for the clusters.**\n",
    "\n",
    "<i style='color: #468fea; font-size: 20px;' class=\"fa fa-code\" aria-hidden=\"true\"></i> In the code block below, calculate the purity scores for your clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a second to reflect on the results of your clustering.**\n",
    "\n",
    "<i style='color: #468fea; font-size: 20px;' class=\"fa fa-file-text\" aria-hidden=\"true\"></i> Which acoustic features result in a better clustering of the emotions and which in a worse clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Clustering Vowels\n",
    "\n",
    "Now that you have analyzed the effectiveness of the acoustic features to capture emotion in the speech files, you will do the same but for vowels. **First, create the same four acoustic features, loudness, pitch, jitter, and MFCCs, for the DAIS dataset. Next, use k-means clustering to cluster the acoustic features. Finally, calculate and analyze the cluster heat maps and cluster purity for your sets of clusters.** Make sure to answer the reflection questions at the end of this section.\n",
    "\n",
    "<i style='color: #468fea; font-size: 20px;' class=\"fa fa-code\" aria-hidden=\"true\"></i> Calculate the acoustic features for the DAIS dataset, cluster the results, using five clusters, and calculate the cluster heat maps and purity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a second to reflect on the results of your clustering.**\n",
    "\n",
    "<i style='color: #468fea; font-size: 20px;' class=\"fa fa-file-text\" aria-hidden=\"true\"></i> Which acoustic features result in a better clustering of the vowels and which in a worse clustering? Are there differences in which acoustic features work better or worse on the two clustering tasks and why do you think that is?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
